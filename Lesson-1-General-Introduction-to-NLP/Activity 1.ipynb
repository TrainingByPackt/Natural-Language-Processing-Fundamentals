{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhaveshb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bhaveshb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhaveshb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhaveshb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import spell\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = open(\"data_ch1/file.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learnning', 'how', 'to', 'pracess', 'Natueral', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "print(words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohom has been corrected to: Soho\n",
      "Ghosh has been corrected to: Ghost\n",
      "learnning has been corrected to: learning\n",
      "pracess has been corrected to: process\n",
      "Natueral has been corrected to: Natural\n",
      "prajects has been corrected to: projects\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence = \"\"\n",
    "corrected_word_list = []\n",
    "for wd in words:\n",
    "    if wd not in string.punctuation:\n",
    "        wd_c = spell(wd)\n",
    "        if wd_c != wd:\n",
    "            print(wd+\" has been corrected to: \"+wd_c)\n",
    "            corrected_sentence = corrected_sentence+\" \"+wd_c\n",
    "            corrected_word_list.append(wd_c)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence+\" \"+wd\n",
    "            corrected_word_list.append(wd)\n",
    "    else:\n",
    "        corrected_sentence = corrected_sentence + wd\n",
    "        corrected_word_list.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP projects. If you want to get early access of it, you should book your order now.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Soho', 'Ghost', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_word_list[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('this', 'DT'), ('book', 'NN'), ('authored', 'VBN'), ('by', 'IN'), ('Soho', 'NNP'), ('Ghost', 'NNP'), ('and', 'CC'), ('Dwight', 'NNP'), ('Gunning', 'NNP'), (',', ','), ('we', 'PRP'), ('shall', 'MD'), ('learning', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('process', 'VB'), ('Natural', 'NNP'), ('Language', 'NNP'), ('and', 'CC'), ('extract', 'JJ'), ('insights', 'NNS'), ('from', 'IN'), ('it', 'PRP'), ('.', '.'), ('The', 'DT'), ('first', 'JJ'), ('four', 'CD'), ('chapter', 'NN'), ('will', 'MD'), ('introduce', 'VB'), ('you', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('basics', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('.', '.'), ('Later', 'NNP'), ('chapters', 'NNS'), ('will', 'MD'), ('describe', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('deal', 'VB'), ('with', 'IN'), ('complex', 'JJ'), ('NLP', 'NNP'), ('projects', 'NNS'), ('.', '.'), ('If', 'IN'), ('you', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('get', 'VB'), ('early', 'JJ'), ('access', 'NN'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('you', 'PRP'), ('should', 'MD'), ('book', 'NN'), ('your', 'PRP$'), ('order', 'NN'), ('now', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(corrected_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insights',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "corrected_word_list_without_stopwords = []\n",
    "for wd in corrected_word_list:\n",
    "    if wd not in stop_words:\n",
    "        corrected_word_list_without_stopwords.append(wd)\n",
    "corrected_word_list_without_stopwords[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'author',\n",
       " 'soho',\n",
       " 'ghost',\n",
       " 'dwight',\n",
       " 'gun',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learn',\n",
       " 'process',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'the',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "corrected_word_list_without_stopwords_stemmed = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
    "corrected_word_list_without_stopwords_stemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Soho',\n",
       " 'Ghost',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'extract',\n",
       " 'insight',\n",
       " '.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'four',\n",
       " 'chapter']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "corrected_word_list_without_stopwords_lemmatized = []\n",
    "for wd in corrected_word_list_without_stopwords:\n",
    "    corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
    "corrected_word_list_without_stopwords_lemmatized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it.', 'The first four chapter will introduce you to the basics of NLP.', 'Later chapters will describe how to deal with complex NLP projects.', 'If you want to get early access of it, you should book your order now.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(corrected_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
